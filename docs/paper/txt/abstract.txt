Deep Learning in raw audio-based musical contexts has received much attention in the last three years, and it is still a rich field for exploration. At the intersection of Deep Learning and audio, some attempts were made for translating deep networks from image to audio applications. DreamSound, an adaptation of the Deep Dream project into an audio-based musical context, is presented as the first translation that uses an audio-trained network, YAMNet. The results are analyzed in detail, and future work is discussed. As an appendix, a python package for DreamSound is presented with its help files.