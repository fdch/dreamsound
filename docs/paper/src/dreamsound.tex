% --------------------------------------------------------------------------
% Template for ICAD-2021 paper; to be used with:
%          icad2021.sty  - ICAD 2021 LaTeX style file, and
%          IEEEbtran.bst - IEEE bibliography style file.
%
% --------------------------------------------------------------------------
\UseRawInputEncoding
\documentclass[a4paper,10pt,oneside]{article}
\usepackage{icad2021,amsmath,epsfig,times,url,hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.15,0.15,0.15}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\def\defeqn{\stackrel{\triangle}{=}}
\newcommand{\symvec}[1]{{\mbox{\boldmath $#1$}}}
\newcommand{\symmat}[1]{{\mbox{\boldmath $#1$}}}

\title{DreamSound: Deep Activation Layer Sonification}

\twoauthors{Federico Camara Halac} {Ohio State University\\ 1813 North High Street\\ Columbus, Ohio  \\ {\tt camarahalac.1@osu.edu}}
{Matias Delgadino} {Oxford University \\ Woodstock Road \\ Oxford, OX2 6GG  \\ {\tt matias.delgadino@maths.ox.ac.uk}}

\begin{document}
\ninept
\maketitle

\begin{sloppy}
\begin{abstract}

\end{abstract}

\section{Introduction}
\label{sec:intro}

Deep Learning (DL) in audio signal processing has received much attention in the last four years, and it is still a growing field.\footnote{See \cite{2019Purwins} for a complete review, \cite{Briot2017} for a published book, as well as\cite{herremans2017proceedings} for the first dedicated workshop, and \cite{choi2017tutorial} and the repositories for DL4M \cite{Bayle2017} and the Aalto courses \cite{Koray2018} for introductiory tutorials. For the use of DL for symbolic music representations, see \cite{yang2017midinet, Briot2018AnET, rachel_manzelli_2018_1492375, hao_wen_dong_2018_1492377, mittal2021symbolic, MuseGanPapers}}  The Music Information Retrieval (MIR) community incorporated DL via large-scale audio datasets that came as early as 2015 \cite{2015piczak,2017audioset,engel2017neural} and most MIR problems were outperformed by DL thenceforth.\footnote{Despite the usefulness of DL in raw audio, e.g. reducing the gap between symbolic and audio classification \cite{sergio_oramas_2017_1417427}, it can be argued that not all MIR problems benefit from it \cite{harsh_verma_2019_3527866}.} 

There exists some musical work at the intersection of DL and audio synthesis, originating with WaveNet \cite{oord2016wavenet}, the first deep convolutional neural network (CNN) for generating raw audio, subsequently with recurrent neural networks (RNN) \cite{mehri2017samplernn, kalchbrenner2018efficient}, but the heavy computational requirements of these first architectures were their main disadvantage. Further optimizations with parallel computations emerged \cite{oord2017parallel,lamtharn_hantrakul_2019_3527860,yamamoto2020parallel,song2021improved}, but it was not until Generative Adversarial Networks (GANs) \cite{Bollepalli_2017, 2017Kaneko, pascual2017segan, 2019waveglow, tian2020tfgan, Liu_2020} that music benefited from DL and audio synthesis directly. Multiple variants to the original WaveGAN \cite{donahue2018adversarial} have appeared in the literature,\footnote{See for example: conditional GAN \cite{2018Lee}, MelGAN \cite{NEURIPS2019_6804c9bc, jang2021universal}, WGANSing \cite{Chandna_2019}, TFGAN \cite{tian2020tfgan}, DRUMGAN \cite{javier_nistal_2020_4245504}, SyleMelGAN \cite{mustafa2021stylemelgan}, among others. In our previous work Kwgan (\url{https://github.com/fdch/kwgan}), we  extended WaveGan with conditionals to tailor an artistically relevant context.} Much of the work around DL and music was fomented by Google's Magenta Lab \cite{adam_roberts_2019_4285266}

These ground-breaking projects were all aimed to train on raw, short-length audio files. A major breakthrough from \cite{oord2016wavenet} and \cite{mehri2017samplernn} comes with the WaveNET-like autoencoder in \cite{engel2017neural}, created as a new, data-driven synthesis technique for longer length audio files, extending GANs towards musical application. Several hybrid synthesizer models have been trained \cite{mccarthy2020hooligan}, and the first GAN synthesizers appeared, such as GANSynth \cite{engel2019gansynth}, trained on the NSynth dataset \cite{engel2017neural}, and EnvGAN \cite{madhu2021envgan}, trained on the ESC dataset \cite{2015piczak} for environmental sound generation. In sum, the evolution of audio synthesis has been fruitful but there is still room for exploration. See \cite{2019Purwins} for a complete overview. 

Historically, most of DL development appeared first on images and then on sound, namely because of the availability of large-scale datasets, e.g. ImageNet \cite{ILSVRC15}. The literature reflects that one inspiration for sonic GANs came from its deep convolutional version \cite{radford2015unsupervised}, an adversarial image generator. The problem of translating networks from images to sounds is mentioned in \cite{RothmanBlog} and \cite{2019Purwins}, and an interesting discussion can be read in \cite{Briot2017}. Of interest here is Deep Dream \cite{Mordvintsev2015}, an image generating architecture. Particularly, \cite{Mordvintsev2015} is geared towards image generation using \textit{activation maximisation} of a layer within a deep network that was previously trained for image classification. 


In the present paper, we present DreamSound\footnote{\url{https://github.com/fdch/dreamsound}}, a creative adaptation of \cite{Mordvintsev2015} to sound using \textit{timbre style transfer} as well as \textit{activation maximisation} DreamSound advances work on DL and audio synthesis, because it is aimed at sonifying layers of a network previously trained on sound, such as YAMNet \cite{YamNet2020}, a novel sound classification deep convolutional network trained on the Audio Set \cite{2017audioset} dataset. Further, DreamSound proposes several creative approaches in relation to layer \textit{activation maximisation}. 
In section 2, we present approach in relation to previous work, as well and the limitations we have found. In section 3, we expose our methods and techniques. In section 4, we present our results and briefly discuss them in context. We conclude in section 5 that there is much work to be done.

% ----------------------------------------------------------------------------
\section{Approach}
% ----------------------------------------------------------------------------

The research in DreamSound can be understood at the intersection of three problems: \textit{input manipulation}, \textit{timbre style transfer}, and \textit{sonification design}. In turn, these problems are described with examples of previous work and their solutions in the present work are presented.


\subsection{Input Manipulation}
The Deep Dream project \cite{Mordvintsev2015} is geared towards image generation using deep layer \textit{activation maximisation}, on a deep network that was trained for image classification on the InceptionNet \cite{szegedy2014going} dataset. At the heart of the algorithm, there is a gradient ascent that takes place between a loss and an input. Loss, in this context, refers to the activations of a layer given some input. The gradient is said to 'ascent' because there is incremental manipulation on the input to a next (feed-forwad) iteration of the same routine. Thus, in time, the input is 'steered' in an additive way towards a particular activation. Briot et al \cite{Briot2017} refer to this type of increments \textit{input manipulation}, since the manipulation occurs at the input, not the output of the architecture.

The output of Deep Dream can be understood as a psychedelic image generator that creates the \textit{pareidolia} effect, i.e., the psychological phenomenon in which the mind, in response to a stimulus, perceives similar patterns where none exist \cite{Briot2017}. Essentially, the effect itself directly depends on the activations of a layer within a deep neural network. Thus, the combination of the neural background and the psychodelic aspect gave way for the \textit{dream} in the name, raising further questions about the nature of artificial networks and their otherness that extend the limits of this paper.

There is existing work in the application of the Deep Dream effect \cite{Mordvintsev2015} to sound. In \cite{Balke2015}, Balke and Dittmar use a magnitude spectrogram as the color channels of an image (RGB) and apply \textit{gradient ascent} between the spectrogram and a deep network trained on images. Subsequently, the output images were resynthesized into sound with Griffin and Lim's method \cite{Lim1983}. Stamenovic adapted a similar approach in the python library tensorflow \cite{Stamenovic2016}. The creative potential of these approaches is radicated in their experimental approach towards spectral transformation. 

In both these cases, the authors perform layer \textit{activation maximisation} on a deep network trained on images, and take the gradient between the image activations and the input. The input, in this case, is the spectrogram of a sound. Therefore, some issues arise due to the usage of images as sound, namely the sequential aspect of audio. For example, in \cite{2019Purwins} Purwins et al note the difference between raw audio (one-dimensional time series signal) and two-dimensional images, and thus claim that audio signals have to be studied sequentially and audio-specific solutions need to be addressed. More specifically, Briot et al claim in \cite{Briot2017}that the \textit{asintropy} of spectrogram representations has been one of the central difficulties when transcoding architectures dealing with images to sounds. That is to say, while the spatial correlation between pixels maintains independently of their direction of an image, this is not the case when dealing with spectrogram images because the horizontal axis represents one thing (e.g. time) and the vertical, another thing (e.g. frequency). 

To address this issue, we implemented an audio-specific solution using raw audio, thus embedding the sequential aspect of audio representations within the architecture. Because there is an incremental manipulation of an initially random content towards a targetted feature space. We chose to perform activation and similarity maximisation via gradient ascent. For details on other types of manipulation, see \cite{Briot2017}

\subsection{Sonification Design}
Inverting features has been a proposed solution \cite{mahendran2014understanding, dosovitskiy2016inverting}, and visualization has been the main channel to understand deep networks \cite{simonyan2014deep}. Particularly in relation to sound is the work of \cite{saumitra_mishra_2018_1492527}, where an important advancement in the interpretation of Machine Listening models was arrived via feature inversion, proving that temporal and harmonic structures are preserved in deep convolutional layers. Following the line of these examples, DreamSound can be understood as a sonification approach geared to better understanding deep networks. In this sense, the results obtained from sonifying the layer activations, i.e., what we call `gradients', are informative of the input but also on the specific layer activation. 

While there are many available trained models for sound classification, we have chosen a general sound source classifier YAMNet \cite{YamNet2020}. YAMNet is a pretrained deep neural network classifier that uses MobileNets \cite{howard2017mobilenets}, a depthwise-separable convolution architecture with a final activation layer that contains most relevant features for classification. The network was previously trained with 521 audio classes based on the Audio  Set corpus \cite{2017audioset}, which are mostly non-musical, short fragments. Therefore, we sacrificed any application of this system with other contextually/musically relevant sonic scenarios to favor a quick presentation of the idea. 

On one hand, YAMNet enabled us a quick prototyping of the DreamSound generator, since it is built and distributed as a tensorflow model within a git repository. On the other, the relatively simple architecture of this network sheds clarity in the process of sound dreaming using neural networks. To this end, our goal is to share our process and preliminary results with the computer music community so as to make deep learning structures easier to grasp. Therefore, two important aspects need to be mentioned. The first is the generality of audio sources within the chosen dataset. The second aspect relates to machine learning sonification. While most machine learning models have a way to visually represent data in graphs, there is very little research into the sonification of this data. Given that our prototipe uses sound as output, our approach is a sonification approach which takes advantage on listening to changes in the audio so as to grasp how a deep activation layer identifies sounds for classification. In fact, we have worked this prototype out with continuous sonic feedback and arduous manipulation throughout.

\subsection{Style Transfer}

\textit{Style transfer} was originally designed for images in \cite{GatysEB15a}, where a deep network is used to obtain content (features) from one image and style (feature correlation) from another. The notion of style represented then the style of an artists. Audio transfer of style does not refer to musical style, but to timbral features. There are other examples dealing with \textit{timbre style transfer} \cite{Foote2016, Ulyanov2016, Wyse2017}, which are dealt with in \cite{Briot2017}, but in general, a gradient-based approach is used on an input and a target in order to synthesize a third, hybrid sound. In \cite{verma2018neural}, Verma and Smith treat audio synthesis as a \textit{style transfer} problem, and they use back-propagation to optimize the sound to conform to filter-outut. 














% -----------------------------------------------------------------------------
\section{Method}
% -----------------------------------------------------------------------------

In general terms, the structure of the Deep Dream process involves inputting an image for classification, grabbing the features obtained of one activation layer during that classification (aka. gradients), and combining the original image with the gradients in some way. In the tensorflow implementation, the gradients are added to the original image in small, gradual increments during a recursive process involving several passes, thus achieving layer \textit{activation maximisation}. In our implementation, we introduced some modifications to the activation maximisation function.

\subsection{Algorithm Overview}

First, we pass a sound file through the model to obtain the activations with respect to one layer and get the loss across all audio frames. Then, we maximize the loss from these activations using gradient ascent with respect to the original sound file. Finally, we combine the gradients with the original sound to output a new, 'dreamed' sound.

\subsection{Calculate Loss}
We pass a sound file (See Figure \ref{fig:img-0}) through the model to obtain its activations (or scores) with respect to one layer (aka, the dream layer). Then, we take the mean accross all frames of the returned activations and return the maximum value which points to one specific class. In other words, we get the class that most represents the sound file so as to find the sum of all of its activations, thus representing the loss of the model with respect to the input.

\subsection{Gradient Ascent}
With the loss, we calcualte its gradient with respect to the input sound file, and maximize the loss so as to excite the features that most represent the input sound file within the network. In this sense, we are motivating the model to resurface what it's learnt. 

\subsection{Recursion}
Once we have the gradients, we can add them or combine them spectrally with the input sound file and listen to the output (See Figures \ref{fig:img-1}, \ref{fig:img-2} and \ref{fig:img-3}). However, if we iterate and recurse, we create a feedback loop that further excites the sonic output and we can define the number of steps and the step size of the recursion (See Figures \ref{fig:img-4}, \ref{fig:img-5}). In other words, we treat this as an feedback delay network effect with amount and amplitude values. Furthermore, we can take multiple layers simultaneously for the dream layers and see how affects the output (See Figures \ref{fig:img-6}, \ref{fig:img-7}).


% -----------------------------------------------------------------------------
\section{Results}
% -----------------------------------------------------------------------------

We have obtained several dreamed sound files with different parameter settings as can be seen on the presented figures across this paper.\footnote{The audio files can be listened to here: \url{https://github.com/fdch/dreamsound}} From these returned sounds, we can assess the validity of dreamsound as a deep dream implementation. In figures \ref{fig:img-1}, \ref{fig:img-2}, and \ref{fig:img-3}, we see the result of one step dream process, with different kinds of combinations between the gradients and the orignal sound. In the first example, we modify the sound usign the same method as in the deep dream tensorflow implementation. 

This means that the gradients are added at some reduced value to the original sound file. From the result, we can listen to a slightly noisier signal. In comparison to the original sound, the noise within this new sound is enhanced when the original sound seems to fall back to silence, and it increments slightly in amplitude towards the middle section of the sound file's lenght. What we listen to here is the activations as they are represented sonically across all audio frames, so that louder portions represent the sections of the file where the most activations exists. This can be more accurately seen in Figure \ref{fig:img-3}, where we obtain the result of only sonifying the activations. However, in Figure \ref{fig:img-2}, we filter the original sound with the activations. This results in a much more insteresting sonic manipulation and result. The activations, since they evolve throghout the soundfile, create a dynamic filter concentrating towards the center of the file, which is when it resonates the most with the original sound file. This means that when we listen to the resonance, the network has the most accurate activations excited. For this step, we implemented a basic FFT filter withi tensorflow:

\begin{lstlisting}[language=Python][caption=A Custom FFT Filter in Tensorflow]
def filt(x,y,w=2048,h=128,m=0.01):
    # take sfft
    X = tf.signal.stft(x,w,h)
    Y = tf.signal.stft(y*m,w,h)
    # power spectrum of Y
    a = tf.math.abs(Y)**2
    # get rid of small values
    s = 0.5*(tf.math.sign(a-0.1)+1)
    a *= s 
    # apply filter
    r = a*tf.math.real(Y)
    i = a*tf.math.imag(Y)
    # convert to complex
    f = tf.dtypes.complex(r,i)
    # add together
    XY = X + f
    # compute the inverse
    out = tf.signal.inverse_stft(XY, w, h)
    
    return out
\end{lstlisting}
When applied recursively, this prototype projects its most promising results. In figures \ref{fig:img-4} and \ref{fig:img-5}, we can listen to the result of 10 iterations of the loss maximization loop acting on its own output. In the fist case, we feed only the gradients back to the loop, so we can see how these get washed out and clustered towards the end of the file. The sonic result reflects a drastic change with respect to the original, which has almost ---but not entirely so--- vanished. It is important to note that the frequency content of the gradient noise, at least aurally, corresponds to that of the original sound file. That is to say, both sounds, while dynamically different, can be catalogued into a related timbre. A more drastic result is in Figure \ref{fig:img-5}, where we feed back into the loop the gradients filtered with the sound file (i.e., a flipping these two), for 10 iterations. This results in a rhythmic variation of the original sound's rhythm. More closely, it sounds very much like Figure \ref{fig:img-3}, but washed and clustered towards the end of the file, as in Figure \ref{fig:img-5}. 

As a final test, we concatenated and summed multiple iterations performed on the last three layers of the network. This resulted in Figures  \ref{fig:img-6} and  \ref{fig:img-7}.


% -------------------------------------------------------------------------
\section{Conclusions and Future Work}
% -------------------------------------------------------------------------

 
We have presented a prototype adaptation of the Deep Dream project into an audio-based musical context using tensorflow and YAMNEt. Our project began as a rapid and remote collaborative work within the Google Colab environment using Python, namely with tensorflow, numpy, and librosa, and it developed into a python package. We have contributed some research at the intersection of Deep Learning and audio-based musical tasks, and presented our results. The most interesting results occur when recursively applying an FFT filter built from the gradients obtained from an input soundfile and its loss. While our approach is tethered to a general sound database and a specific model, the techniques of gradient ascent for loss maximization of layer activations, and the FFT filter exposed in this paper are be portable enought for importing into other existing classification models for audio, such as for genre or musical instrument classification. Further work includes plans to extend this project towards the creation of a database of dreamed sounds, with the capability to change models and other fft filtering techniques, as well as other modifications using GANs.

\section{Acknowledgments}
The authors would like to thank the reviewers, adding that the camera-ready paper will have a more detailed and verbose bibliographic section.

\bibliographystyle{IEEEtran}
\bibliography{dreamsound.bib}

\end{sloppy}
\end{document}




% Some examples within the last four years are: 
% genre classification \cite{sergio_oramas_2017_1417427}, 
% singing voice separation \cite{andreas_jansson_2017_1414934}, 
% instrument activity detection \cite{siddharth_gururani_2018_1492479}, 
% instrument classification \cite{juan_s_gomez_2018_1492481}, 
% downbeat tracking \cite{magdalena_fuentes_2018_1492355}, 
% optical music recognition \cite{lukas_tuggener_2018_1492401}, 
% mood detection \cite{remi_delbouys_2018_1492427}, 
% tempo estimation \cite{hadrien_foroughmand_2019_3527890},
% key classification \cite{fu_zih_sing_2019_3527960}, 
% beat-tracking \cite{akira_maezawa_2017_1415520,magdalena_fuentes_2019_3527792}, 
% sequence classification \cite{sathwik_tejaswi_madhusudhan_2019_3527862}, 
% music analogy \cite{ruihan_yang_2019_3527880}, 
% drum transcription \cite{keunwoo_choi_2019_3527774}, 
% mood classification \cite{filip_korzeniowski_2020_4245488}, 
% source separation \cite{petermann2020deep}, 
% unison singing \cite{pritish_chandna_2020_4245502},

% voice separation in symbolic music \cite{reinier_de_valk_2018_1492403}, 

